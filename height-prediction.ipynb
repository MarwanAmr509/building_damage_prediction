{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install skorch"]},{"cell_type":"markdown","metadata":{},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Importing necessary libraries\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import random\n","import time\n","import string\n","from datetime import datetime\n","import itertools\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from scipy.sparse import hstack, csr_matrix\n","from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n","from sklearn.compose import ColumnTransformer\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,RandomForestClassifier\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.kernel_ridge import KernelRidge\n","from sklearn.linear_model import (ElasticNet, Lasso, BayesianRidge, LassoLarsIC, Ridge,\n","                                  LogisticRegression)\n","import sklearn.metrics\n","from sklearn.metrics import (confusion_matrix, roc_curve, auc, accuracy_score, \n","                             f1_score, precision_score, recall_score,classification_report)\n","from sklearn.model_selection import (KFold, cross_val_score, train_test_split, \n","                                     GridSearchCV)\n","from sklearn.pipeline import Pipeline, make_pipeline\n","from sklearn.preprocessing import (LabelEncoder, OneHotEncoder, RobustScaler, \n","                                   StandardScaler, Normalizer)\n","from sklearn.svm import LinearSVC\n","\n","import xgboost as xgb\n","import lightgbm as lgb\n","from mlxtend.regressor import StackingRegressor, StackingCVRegressor\n","\n","from sklearn.impute import SimpleImputer\n","\n","from imblearn.pipeline import Pipeline as imblearnPipeline\n","\n","\n","from imblearn.over_sampling import SMOTE\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","from skorch import NeuralNetClassifier\n","\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","from xgboost import XGBRegressor\n","\n","from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n","\n","\n","\n","\n","import joblib\n","import pickle\n","import nltk\n","\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_values = pd.read_csv('/kaggle/input/clean-data/Clean Data/train_values.csv')\n","train_labels = pd.read_csv('/kaggle/input/clean-data/Clean Data/train_labels.csv')\n","test_values = pd.read_csv('/kaggle/input/clean-data/Clean Data/test_values.csv')\n","test_labels = pd.read_csv('/kaggle/input/clean-data/Clean Data/test_labels.csv')\n","\n","train_values['Labels'] = train_labels['Labels']\n","test_values['Labels'] = test_labels['Labels']\n","\n","# Combine train_values and test_values into one DataFrame\n","df = pd.concat([train_values, test_values], ignore_index=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting the distribution of height for pre and post earthquake\n","plt.figure(figsize=(14, 6))\n","\n","# Pre-earthquake height distribution\n","plt.subplot(1, 2, 1)\n","plt.hist(df['height_ft_pre_eq'], bins=30, color='blue', alpha=0.7)\n","plt.title('Height Distribution Pre-Earthquake')\n","plt.xlabel('Height (ft)')\n","plt.ylabel('Frequency')\n","\n","# Post-earthquake height distribution\n","plt.subplot(1, 2, 2)\n","plt.hist(df['height_ft_post_eq'], bins=30, color='red', alpha=0.7)\n","plt.title('Height Distribution Post-Earthquake')\n","plt.xlabel('Height (ft)')\n","plt.ylabel('Frequency')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.set_index('building_id', inplace=True)\n","\n","df=df.drop(['Labels','geo1','geo2','geo3'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Separate features and target variable\n","X = df.drop(['height_ft_post_eq','count_floors_post_eq'], axis=1)\n","y = df['height_ft_post_eq']\n","\n","# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print('Data in Train dataset:', len(X_train))\n","print('Data in Test dataset:', len(X_test))"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define numeric and categorical features\n","num_feat = X.select_dtypes(include=['int64', 'float64']).columns\n","cat_feat = X.select_dtypes(include=['object']).columns\n","\n","# Create the preprocessing steps for numeric features\n","numeric_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n","    ('scaler', StandardScaler())])\n","\n","# Create the preprocessing steps for categorical features\n","categorical_transformer = Pipeline(steps=[\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n","\n","# Combine preprocessing steps\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, num_feat),\n","        ('cat', categorical_transformer, cat_feat)])"]},{"cell_type":"markdown","metadata":{},"source":["## XGBoost Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the pipeline with preprocessing and XGBoost regressor\n","pipeline = Pipeline(steps=[\n","    ('preprocessor', preprocessor),\n","    ('regressor', XGBRegressor(random_state=42))\n","])\n","\n","# Hyperparameter tuning with Grid Search\n","params = {\n","    'regressor__max_depth': [3, 5, 7],\n","    'regressor__n_estimators': [50, 100, 200]\n","}\n","\n","grid = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1, verbose=1)\n","\n","\n","start_time = time.time()\n","\n","\n","# Train the model\n","grid.fit(X_train, y_train)\n","\n","\n","end_time = time.time()\n","training_time = end_time - start_time\n","\n","# Get the best model\n","best_xgb_model = grid.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict on the test set\n","y_pred = pipeline.predict(X_test)\n","\n","# Calculate performance metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f'Mean Squared Error: {mse}')\n","print(f'R^2 Score: {r2}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def regression_evaluation_report(y_true, y_pred):\n","    \"\"\"\n","    Prints an evaluation report for regression models.\n","    \n","    Parameters:\n","        y_true (array-like): True target values.\n","        y_pred (array-like): Predicted target values.\n","    \"\"\"\n","    # Calculate metrics\n","    # Calculate metrics\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    n = len(y_true)\n","    p = len(y_pred[0]) if len(y_pred.shape) > 1 else 1\n","    adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n","    \n","    # Print metrics\n","    print(\"Regression Evaluation Report\")\n","    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n","    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n","    print(f\"R-squared (RÂ²): {r2:.4f}\")\n","    print(f\"Adjusted R-squared: {adjusted_r2:.4f}\")\n","    \n","    # Plot True vs Predicted values\n","    plt.figure(figsize=(8, 6))\n","    plt.scatter(y_true, y_pred, alpha=0.3)\n","    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color='red', linestyle='--', linewidth=2)\n","    plt.xlabel('True Values')\n","    plt.ylabel('Predicted Values')\n","    plt.title('True vs Predicted Values')\n","    plt.show()\n","    \n","    # Plot Residuals\n","    residuals = y_true - y_pred\n","    plt.figure(figsize=(8, 6))\n","    sns.histplot(residuals, kde=True)\n","    plt.xlabel('Residuals')\n","    plt.ylabel('Frequency')\n","    plt.title('Residuals Distribution')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["regression_evaluation_report(y_test, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Save the model\n","xgb_model_filename = 'height_prediction_xgboost_regressor_55_model.joblib'\n","joblib.dump(best_xgb_model, xgb_model_filename)\n","\n","print(f'XGBoost Model saved as {xgb_model_filename}')"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the pipeline with preprocessing and XGBoost regressor\n","pipeline = Pipeline(steps=[\n","    ('preprocessor', preprocessor),\n","    ('regressor', XGBRegressor(random_state=42))\n","])\n","\n","# Hyperparameter tuning with Grid Search\n","params = {\n","    'regressor__max_depth': [ 7, 9, 11],\n","    'regressor__n_estimators': [200, 300, 500],\n","    'regressor__learning_rate': [0.05, 0.1],\n","    'regressor__subsample': [ 0.8, 1.0],\n","\n","}\n","\n","\n","grid = GridSearchCV(pipeline, param_grid=params, cv=5, n_jobs=-1, verbose=1)\n","\n","\n","start_time = time.time()\n","\n","\n","# Train the model\n","grid.fit(X_train, y_train)\n","\n","\n","end_time = time.time()\n","training_time = end_time - start_time\n","\n","# Get the best model\n","best_xgb_model = grid.best_estimator_\n","best_params = grid.best_params_\n","\n","# Print the best hyperparameters\n","print(\"Best Parameters:\", best_params)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict on the test set\n","y_pred = grid.predict(X_test)\n","\n","# Calculate performance metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f'Mean Squared Error: {mse}')\n","print(f'R^2 Score: {r2}')\n","print('time:',training_time/60)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["regression_evaluation_report(y_test, y_pred)"]},{"cell_type":"markdown","metadata":{},"source":["## Random Forest Regressor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the pipeline with preprocessing and RandomForestRegressor\n","pipeline = Pipeline(steps=[\n","    ('preprocessor', preprocessor),\n","    ('regressor', RandomForestRegressor(random_state=42))\n","])\n","\n","# Train the model\n","pipeline.fit(X_train, y_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict on the test set\n","y_pred = pipeline.predict(X_test)\n","\n","# Calculate performance metrics\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f'Mean Squared Error: {mse}')\n","print(f'R^2 Score: {r2}')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["regression_evaluation_report(y_test, y_pred)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
