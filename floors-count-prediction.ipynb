{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install skorch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# General libraries\nimport random\nimport time\nimport string\nfrom datetime import datetime\nimport itertools\n\n# Data manipulation and analysis\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning and preprocessing\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC, Ridge, LogisticRegression\nfrom sklearn.metrics import (confusion_matrix, roc_curve, auc, accuracy_score, \n                             f1_score, precision_score, recall_score, classification_report,\n                             mean_squared_error, r2_score, mean_absolute_error)\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler, StandardScaler, Normalizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.impute import SimpleImputer\n\n# Specialized machine learning libraries\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom mlxtend.regressor import StackingRegressor, StackingCVRegressor\n\n# Imbalanced learning\nfrom imblearn.pipeline import Pipeline as imblearnPipeline\nfrom imblearn.over_sampling import SMOTE\n\n# Deep learning\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom skorch import NeuralNetClassifier\n\n# Saving and loading models\nimport joblib\nimport pickle\n\n# Natural language processing\nimport nltk\n\n# Set plotting to inline mode\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_values = pd.read_csv('/kaggle/input/clean-data/Clean Data/train_values.csv')\ntrain_labels = pd.read_csv('/kaggle/input/clean-data/Clean Data/train_labels.csv')\ntest_values = pd.read_csv('/kaggle/input/clean-data/Clean Data/test_values.csv')\ntest_labels = pd.read_csv('/kaggle/input/clean-data/Clean Data/test_labels.csv')\n\ntrain_values['Labels'] = train_labels['Labels']\ntest_values['Labels'] = test_labels['Labels']\n\n# Combine train_values and test_values into one DataFrame\ndf = pd.concat([train_values, test_values], ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the distribution of floors count for pre and post earthquake\nplt.figure(figsize=(14, 6))\n\n# Pre-earthquake floors count distribution\nplt.subplot(1, 2, 1)\nplt.hist(df['count_floors_pre_eq'], bins=10, color='blue', alpha=0.7)\nplt.title('Floors Count Distribution Pre-Earthquake')\nplt.xlabel('Number of Floors')\nplt.ylabel('Frequency')\nplt.grid(axis='y', linestyle='--', alpha=0.7)  # Horizontal grid lines only\n\n# Post-earthquake floors count distribution\nplt.subplot(1, 2, 2)\nplt.hist(df['count_floors_post_eq'], bins=10, color='red', alpha=0.7)\nplt.title('Floors Count Distribution Post-Earthquake')\nplt.xlabel('Number of Floors')\nplt.ylabel('Frequency')\nplt.grid(axis='y', linestyle='--', alpha=0.7)  # Horizontal grid lines only\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate features and target variable\nX = df.drop(['height_ft_post_eq','count_floors_post_eq'], axis=1)\ny = df['height_ft_post_eq']\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint('Data in Train dataset:', len(X_train))\nprint('Data in Test dataset:', len(X_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"# Define numeric and categorical features\nnum_feat = X.select_dtypes(include=['int64', 'float64']).columns\ncat_feat = X.select_dtypes(include=['object']).columns\n\n# Create the preprocessing steps for numeric features\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=-999)),\n    ('scaler', StandardScaler())])\n\n# Create the preprocessing steps for categorical features\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, num_feat),\n        ('cat', categorical_transformer, cat_feat)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Create a complete pipeline with SMOTE and Logistic Regression classifier\npipeline = imblearnPipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('smote', SMOTE(random_state=42)),  # Handle class imbalance\n    ('classifier', LogisticRegression(random_state=42))])\n\n# Fit the model\npipeline.fit(X_train, y_train)\n\n# Evaluate the model\n\ntrain_score = pipeline.score(X_train, y_train)\ntest_score = pipeline.score(X_test, y_test)\ny_pred = pipeline.predict(X_test)\nf1 = f1_score(y_test, y_pred, average='micro')\n\nprint(\"Train score: %.3f\" % train_score)\nprint(\"Test score: %.3f\" % test_score)\nprint(\"F1 Score: %.3f\" % f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_classification_report(y_true, y_pred):\n    \"\"\"\n    Prints a classification report for given true and predicted outputs.\n    \n    Parameters:\n        y_true (array-like): True labels.\n        y_pred (array-like): Predicted labels.\n    \"\"\"\n    # Calculate metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n    f1_micro = f1_score(y_true, y_pred, average='micro')  # Micro F1 Score\n    \n    # Print classification report from sklearn\n    class_report = classification_report(y_true, y_pred, digits=2)\n    print(\"Detailed Classification Report\")\n    print(class_report)\n    \n    # Print additional metrics\n    print(\"Overall Metrics:\")\n    print(f\"Accuracy: {accuracy:.2f}\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 Score (Weighted): {f1_weighted:.2f}\")\n    print(f\"F1 Score (Micro): {f1_micro:.2f}\")  # Display Micro F1 Score\n    \n    # Print and plot confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    print(\"Confusion Matrix:\")\n    print(cm)\n    \n    # Plotting using seaborn\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=True, yticklabels=True)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title('Confusion Matrix')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluating the model\nprint_classification_report(y_test, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel_filename = 'count_floors_logistic_regression_84_model.joblib'\njoblib.dump(pipeline, model_filename)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"pipeline = imblearnPipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('smote', SMOTE(random_state=42)),  # Handle class imbalance\n    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))\n])\n\n\n# Fit the model\npipeline.fit(X_train, y_train)\n\n# Make predictions\ny_pred = pipeline.predict(X_test)\n\n# Calculate scores\ntrain_score = pipeline.score(X_train, y_train)\ntest_score = pipeline.score(X_test, y_test)\nf1 = f1_score(y_test, y_pred, average='micro')\n\n# Print scores\nprint(\"Train score: %.3f\" % train_score)\nprint(\"Test score: %.3f\" % test_score)\nprint(\"F1 Score: %.3f\" % f1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluating the model\nprint_classification_report(y_test, y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model\nmodel_filename = 'count_floors_random_forest_87__model.joblib'\njoblib.dump(pipeline, model_filename)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deep Neural Netwrok (DNN)","metadata":{}},{"cell_type":"code","source":"# Preprocessing pipeline\npreprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n\n# Apply preprocessing\nX_processed = preprocessing_pipeline.fit_transform(X)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DNNClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size_1, hidden_size_2, output_size):\n        super(DNNClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size_1)\n        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)\n        self.fc3 = nn.Linear(hidden_size_2, output_size)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n\n# Convert them to PyTorch tensors and create DataLoader objects\ntrain_dataset = TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train.values))\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = TensorDataset(torch.Tensor(X_test), torch.LongTensor(y_test.values))\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# Define hyperparameters\ninput_size = X_train.shape[1]  # Number of input features\nhidden_size_1 = 64  # Number of neurons in the first hidden layer\nhidden_size_2 = 32  # Number of neurons in the second hidden layer\noutput_size = 10  # Number of classes (0 to 9 floors)\n\n# Instantiate the DNN model\nmodel = DNNClassifier(input_size, hidden_size_1, hidden_size_2, output_size)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for multi-class classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {loss.item():.4f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\nmodel.eval()\ncorrect_test = 0\ntotal_test = 0\ntrue_labels = []\npredicted_labels = []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total_test += labels.size(0)\n        correct_test += (predicted == labels).sum().item()\n        true_labels.extend(labels.numpy())\n        predicted_labels.extend(predicted.numpy())\n\ntesting_accuracy = correct_test / total_test\nprint(f\"Test Accuracy: {testing_accuracy:.4f}\")\n\n# Calculate and print detailed metrics for the test set\naccuracy = accuracy_score(true_labels, predicted_labels)\nprecision = precision_score(true_labels, predicted_labels, average='weighted')\nrecall = recall_score(true_labels, predicted_labels, average='weighted')\nf1_weighted = f1_score(true_labels, predicted_labels, average='weighted')\nf1_micro = f1_score(true_labels, predicted_labels, average='micro')\n\n# Print classification report from sklearn\nclass_report = classification_report(true_labels, predicted_labels, digits=2)\nprint(\"Detailed Classification Report\")\nprint(class_report)\n\n# Print additional metrics\nprint(\"Overall Metrics:\")\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score (Weighted): {f1_weighted:.2f}\")\nprint(f\"F1 Score (Micro): {f1_micro:.2f}\")\n\n# Print and plot confusion matrix\ncm = confusion_matrix(true_labels, predicted_labels)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Plotting using seaborn\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=True, yticklabels=True)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}